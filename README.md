### Survey Text Analysis Tool

A Python tool that ingests a survey CSV, translates open-text answers, uses an LLM to generate themes, assigns every response to a theme for exact counts, and produces charts and a markdown report. Data is stored in SQLite so runs are repeatable and auditable.

Works great for customer, employee, or product feedback surveys with open-ended questions.

â¸»

âœ¨ Features
	â€¢	CSV â†’ SQLite import with dedupe and basic data validation
	â€¢	Translation to English (DeepL preferred; Google Translate fallback)
	â€¢	LLM-powered theming
	â€¢	Proposes 4â€“8 concise themes per question
	â€¢	Classifies every response into a theme (stored in theme_assignments)
	â€¢	Representative quotes with respondent name/company attribution
	â€¢	Charts (PNG) and Markdown report summarizing themes and counts
	â€¢	CLI modes to run the full pipeline or individual steps
	â€¢	Non-interactive mode for automation/CI

â¸»

ğŸ“¦ Requirements
	â€¢	Python 3.10â€“3.12 recommended
(Python 3.13 may break googletrans; prefer DeepL or deep-translator)

Install dependencies

Minimal set (required):

pip install pandas matplotlib python-dotenv

LLM (recommended):

pip install "openai>=1.45.0"

Translation (choose one):

# DeepL (recommended; requires API key)
pip install deepl

# or: googletrans (works best < Python 3.13)
pip install googletrans==4.0.0-rc1

Optional alternative translator (if on Python 3.13):

pip install deep-translator

(Youâ€™d need a small code tweak to use deep-translator instead of googletrans.)

Homebrew/Python note (macOS): If you hit externally-managed-environment errors (PEP 668), use a virtualenv, pyenv Python, or add --break-system-packages to pip. Best practice is a venv:

python3 -m venv .venv
source .venv/bin/activate


â¸»

ğŸ” Environment Variables

Create a .env file (optional) in the project root:

OPENAI_API_KEY=sk-...
DEEPL_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxx
ASSUME_YES=1

	â€¢	OPENAI_API_KEY â€” required for LLM theming
	â€¢	DEEPL_API_KEY â€” optional; enables higher-quality translations
	â€¢	ASSUME_YES â€” optional; non-interactive mode (skip prompts)

â¸»

ğŸ“ Expected CSV Columns

Configure via config if your column names differ. Defaults:
	â€¢	Satisfaction column: Overall, how satisfied are you with ...?
	â€¢	Open text:
	â€¢	What is the main reason for your satisfaction rating?
	â€¢	If ... could change one thing to make your job easier, what would it be?
	â€¢	Is there anything you wish ... offered that we donâ€™t today?
	â€¢	If you had 30 seconds with the executive team, what would you tell them?
	â€¢	What changes in your industry are worrying you or keeping you up at night?
	â€¢	Attribution:
	â€¢	Contact Name
	â€¢	Contact Account Name
	â€¢	Contact Email

The first open-text question (â€œmain reasonâ€) is segmented by satisfaction; others are analyzed across all respondents.

â¸»

ğŸš€ Usage

Full pipeline

python survey_analyzer.py --csv "./survey.csv" --full --assume-yes

Step-by-step

# Import + translate only
python survey_analyzer.py --csv "./survey.csv" --translate-only

# Import + translate + analyze (no charts/reports)
python survey_analyzer.py --csv "./survey.csv" --analyze-only

# Charts + report from existing DB
python survey_analyzer.py --csv "./survey.csv" --reports-only

Options

--db <file>                SQLite DB path (default: cpm_survey.db)
--db-action replace|append Replace DB or append new rows on import
--assume-yes               Non-interactive mode (skip prompts)

During startup you can select an OpenAI model (default is a cost-efficient option). Models named gpt-5* may require the Responses API; the script uses Chat Completions and will still run with prior models (e.g., gpt-4o-mini).

â¸»

ğŸ—„ï¸ Data Model (SQLite)
	â€¢	survey_responses â€” raw + translated text, satisfaction, contact fields
	â€¢	themes â€” theme label/description per question (+segment) with one representative quote and attribution
	â€¢	theme_assignments â€” one row per responseâ†’theme (source of truth for counts)
	â€¢	analysis_runs â€” basic run metadata

Exact counts and charts are derived from theme_assignments (not LLM guesses).

â¸»

ğŸ“¤ Outputs
	â€¢	Charts (PNG): reports/
	â€¢	Stacked bar: main reason by satisfaction
	â€¢	Top themes: other questions
	â€¢	Markdown report: cpm_survey_analysis.md
	â€¢	Summary stats, per-question themes with counts
	â€¢	One representative quote per theme (with name/company if available)

â¸»

ğŸ”§ Configuration (advanced)

You can pass a config dict (or extend the script) to override:

config = {
  "ai_model": "gpt-4o-mini",
  "ai_max_retries": 3,
  "ai_base_delay": 0.1,
  "min_responses": 10,
  "large_dataset_threshold": 5000,
  # Column overrides:
  # "satisfaction_column": "...",
  # "name_column": "...",
  # "email_column": "...",
  # "company_column": "...",
}


â¸»

ğŸ”’ Privacy & PII
	â€¢	Representative quotes include the respondent name and company if available.
	â€¢	If you need redaction (e.g., â€œFirst L., Companyâ€), add a small post-processing step before writing reports.



ğŸ™Œ Acknowledgments

Built to make open-text survey analysis fast, repeatable, and auditable â€” with exact counts, clear quotes, and minimal fuss.
